{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constructing data set with sepsis features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['HR', 'O2Sat', 'SBP', 'MAP', 'DBP', 'Age', 'Gender', 'HospAdmTime', 'ICULOS', 'SepsisLabel']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from os import listdir\n",
    "\n",
    "path = \"../training/\"\n",
    "paths = [path + \"p0\" + str(10000+i)[1:] + \".psv\" for i in range(1, 5001)]\n",
    "data = np.array([np.loadtxt(open(path + file), delimiter='|', skiprows=1) for file in paths])\n",
    "keys = open(paths[0]).readline().rstrip().split('|')\n",
    "\n",
    "def delete_columns_nan(data,keys):\n",
    "    df = {}\n",
    "    for i, column in enumerate(data.T):\n",
    "        if not np.isnan(column).all():\n",
    "            df[keys[i]] = column\n",
    "    return df\n",
    "\n",
    "\n",
    "def replace_nan_by_value(data, value=None):\n",
    "    for j, patient in enumerate(data):\n",
    "        for key in patient.keys():\n",
    "            if value == 'normal':\n",
    "                for i in range(len(patient[key])):\n",
    "                    p = patient[key].copy()[~np.isnan(patient[key])]\n",
    "                    if np.isnan(patient[key][i]):\n",
    "                        if value == 'mean':\n",
    "                            data[j][key][i] = np.mean(p)\n",
    "                        if value == 'normal':\n",
    "                            data[j][key][i] = np.random.normal(np.mean(p), np.std(p))\n",
    "            else:\n",
    "                patient[key] = np.nan_to_num(patient[key])\n",
    "    return data\n",
    "\n",
    "data_aux = []\n",
    "for patient in data:\n",
    "    data_aux.append(delete_columns_nan(patient, keys))\n",
    "\n",
    "# Count non-EMPTY entries throughout patients\n",
    "df = {}\n",
    "for key in keys:\n",
    "    df[key] = 0\n",
    "\n",
    "for patient in data_aux:\n",
    "    for key in patient.keys():\n",
    "        df[key] += 1\n",
    "    \n",
    "    \n",
    "data_new = replace_nan_by_value(data_aux, None)\n",
    "\n",
    "data_aux = []\n",
    "sepsis_keys = {}\n",
    "df = {}\n",
    "\n",
    "for key in keys:\n",
    "    df[key] = 0\n",
    "    sepsis_keys[key] = 0\n",
    "\n",
    "for patient in data:\n",
    "    data_aux.append(delete_columns_nan(patient, keys))\n",
    "    if np.any(data_aux[-1]['SepsisLabel']) == 1:\n",
    "        for key in data_aux[-1].keys():\n",
    "            sepsis_keys[key] += 1\n",
    "\n",
    "keys_s = []\n",
    "for key in sepsis_keys.keys():\n",
    "    if sepsis_keys[key] == np.max(list(sepsis_keys.values())):\n",
    "        keys_s.append(key)\n",
    "print(keys_s)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New Data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_dataset = []\n",
    "for patient in data_new:\n",
    "    if len(keys_s & patient.keys()) == len(keys_s):\n",
    "        new_dataset.append([patient[d] for d in keys_s])\n",
    "new_dataset = np.hstack(new_dataset).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Begin Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2623\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier as rf\n",
    "from sklearn.model_selection import LeaveOneOut, KFold, ShuffleSplit\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier as knn\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "x = new_dataset[:,:-1]\n",
    "scaler = scaler.fit(x)\n",
    "x = scaler.transform(x)\n",
    "\n",
    "y = new_dataset[:,-1]\n",
    "print(len(np.where(y==1)[0]))\n",
    "# x_under_sampled = [i for i in x[np.where(y!=1)[0]]]\n",
    "# x_under_sampled.append(x[np.where(y==1)[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SMOTE\n",
    "\n",
    "Smote is an oversampling technique used to balance unbalanced data. In this case it oversamples the class with less number of samples.<br>\n",
    "We could also undersample the class with the highest number of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "x, y = SMOTE().fit_resample(x, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Principal Component Analysis\n",
    "PCA allows to find components that carry most of the information and to reduce data dimensionality, hopefully, improving the classifiers' results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(370650, 9)\n",
      "(370650, 5)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "print(x.shape)\n",
    "\n",
    "pca = PCA(.95)\n",
    "x = pca.fit_transform(x)\n",
    "\n",
    "print(x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [ 37065  37066  37067 ... 370647 370648 370649] TEST: [    0     1     2 ... 37062 37063 37064]\n",
      "0.20842017507294708\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [37065 37066 37067 ... 74127 74128 74129]\n",
      "0.20734962020119072\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [ 74130  74131  74132 ... 111192 111193 111194]\n",
      "0.22850729517396184\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [111195 111196 111197 ... 148257 148258 148259]\n",
      "0.20278330019880716\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [148260 148261 148262 ... 185322 185323 185324]\n",
      "0.19854401058901391\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [185325 185326 185327 ... 222387 222388 222389]\n",
      "0.9663705205414107\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [222390 222391 222392 ... 259452 259453 259454]\n",
      "0.9669012612361508\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [259455 259456 259457 ... 296517 296518 296519]\n",
      "0.9670452170277991\n",
      "TRAIN: [     0      1      2 ... 370647 370648 370649] TEST: [296520 296521 296522 ... 333582 333583 333584]\n",
      "0.9679224793250356\n",
      "TRAIN: [     0      1      2 ... 333582 333583 333584] TEST: [333585 333586 333587 ... 370647 370648 370649]\n",
      "0.9684682177446288\n"
     ]
    }
   ],
   "source": [
    "loo = LeaveOneOut()\n",
    "kf = KFold(n_splits=10)\n",
    "\n",
    "for train_index, test_index in kf.split(x):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "    random_forest = rf(n_estimators=1)\n",
    "    random_forest = random_forest.fit(X_train, y_train)\n",
    "    results = random_forest.predict(X_test)\n",
    "    print(f1_score(y_test, results))\n",
    "    \n",
    "#     svm_c = svm.OneClassSVM(gamma='auto', verbose=True, max_iter=1)\n",
    "#     svm_c = svm_c.fit(X_train, y_train)\n",
    "#     results = svm_c.predict(X_test)\n",
    "#     print(f1_score(y_test, results, average='micro'))\n",
    "    \n",
    "#     knear = knn()\n",
    "#     knear = knear.fit(X_train, y_train)\n",
    "#     results = knear.predict(X_test)\n",
    "#     print(f1_score(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9388641575610415\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAIN: [223808  82199  59933 ... 157660 357773  90437] TEST: [122930 264761  35758 ... 342925 303203   5343]\n",
      "0.602131991855312\n",
      "TRAIN: [162475  38981 107818 ... 345121 252108 194569] TEST: [  4849 290585 194297 ...  33959 294669 220344]\n",
      "0.5963226126638663\n",
      "TRAIN: [  3562   7524 366351 ...  32224  57594  94626] TEST: [ 92360  63935 201089 ... 362877 112286 169202]\n",
      "0.5790783801833527\n",
      "TRAIN: [276697 222825  54815 ...  58307 227993  66758] TEST: [318370 195007 190635 ...  95257  14434  48999]\n",
      "0.5821330568937131\n",
      "TRAIN: [140706  73401 106162 ... 206222 218703   2706] TEST: [ 44606 206898 283930 ...  47980 177878 226414]\n",
      "0.5800201767286959\n",
      "TRAIN: [288933 363863 272781 ... 332276  63240  53216] TEST: [ 78492 156968 253257 ...  38538 133873   4323]\n",
      "0.5766790155155339\n",
      "TRAIN: [184789   7438 281403 ... 260475 369106  43280] TEST: [356809 370315 105677 ... 214439  65198 152947]\n",
      "0.5899968604342262\n",
      "TRAIN: [333591 138164 315998 ...  29126 133346 322599] TEST: [244871 213903 116223 ... 114231 245832  27557]\n",
      "0.5913901410675447\n",
      "TRAIN: [238361 163867 345552 ...   3840 167054  33818] TEST: [ 90735 171760 111303 ...  43397 305353 323477]\n",
      "0.6015453066692182\n",
      "TRAIN: [ 96920 216545  99307 ...  34015 111129 148560] TEST: [ 41062  56401 117349 ... 333943  38802 137895]\n",
      "0.5794622216814698\n"
     ]
    }
   ],
   "source": [
    "rs = ShuffleSplit(n_splits=10, train_size=0.5, test_size=.25, random_state=0)\n",
    "\n",
    "for train_index, test_index in rs.split(x):\n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    X_train, X_test = x[train_index], x[test_index]\n",
    "    y_train, y_test = y[train_index], y[test_index]\n",
    "    \n",
    "#     random_forest = rf(n_estimators=1000)\n",
    "#     random_forest = random_forest.fit(X_train, y_train)\n",
    "#     results = random_forest.predict(X_test)\n",
    "#     print(f1_score(y_test, results))\n",
    "    clf = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(x.shape[1], 1), random_state=42)\n",
    "    clf = clf.fit(X_train, y_train)\n",
    "    results = clf.predict(X_test)\n",
    "    print(f1_score(y_test, results))\n",
    "    \n",
    "#     svm_c = svm.OneClassSVM(gamma='auto', verbose=True, max_iter=1)\n",
    "#     svm_c = svm_c.fit(X_train, y_train)\n",
    "#     results = svm_c.predict(X_test)\n",
    "#     print(f1_score(y_test, results, average='micro'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6269924349524622\n"
     ]
    }
   ],
   "source": [
    "print(accuracy_score(y_test, results))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
